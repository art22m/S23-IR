{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space with ML\n",
    "\n",
    "This lab will be devoted to the use of ML model for the needs of information retrieval and text classification.  \n",
    "\n",
    "**Searching in the curious facts database**\n",
    "\n",
    "The facts dataset is given [here](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), take a look. We want you to retrieve facts **relevant to the query** (whatever it means), for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use neural networks to embed sentences\n",
    "\n",
    "Make use of any, starting from doc2vec up to Transformers, etc. Provide all code, dependencies, installation requirements.\n",
    "\n",
    "\n",
    "- [UCE in spacy 2](https://spacy.io/universe/project/spacy-universal-sentence-encoder) (`!pip install spacy-universal-sentence-encoder`)\n",
    "- [Sentence BERT in spacy 2](https://spacy.io/universe/project/spacy-sentence-bert) (`!pip install spacy-sentence-bert`)\n",
    "- [Pretrained ðŸ¤— Transformers](https://huggingface.co/transformers/pretrained_models.html)\n",
    "- [Spacy 3 transformers](https://spacy.io/usage/embeddings-transformers#transformers-installation)\n",
    "- [doc2vec pretrained](https://github.com/jhlau/doc2vec)\n",
    "- [Some more sentence transformers](https://www.sbert.net/docs/quickstart.html)\n",
    "- [Even fasttext can do a sentence embedding](https://fasttext.cc/docs/en/python-module.html#model-object)\n",
    "\n",
    "Here should be dependency installation, download instructions and so on. With outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting spacy-universal-sentence-encoder\n",
      "  Downloading spacy_universal_sentence_encoder-0.4.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<3.0.0,>=2.4.0 in /usr/local/lib/python3.9/site-packages (from spacy-universal-sentence-encoder) (2.10.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.9/site-packages (from spacy-universal-sentence-encoder) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.9/site-packages (from spacy-universal-sentence-encoder) (0.12.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.23.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.10.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (65.6.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.12)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (8.1.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/site-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.0.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.27.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (22.9.24)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.4.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.3.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.10.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.50.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/site-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.19.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.38.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2022.9.24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.13.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.2.2)\n",
      "Building wheels for collected packages: spacy-universal-sentence-encoder\n",
      "  Building wheel for spacy-universal-sentence-encoder (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spacy-universal-sentence-encoder: filename=spacy_universal_sentence_encoder-0.4.5-py3-none-any.whl size=15793 sha256=17b573fa6085a41904ee5c0ff173398fb16af6fc8052573a2fca05a113e5b489\n",
      "  Stored in directory: /Users/artmurashko/Library/Caches/pip/wheels/78/d2/ff/4c091ddba84486e45a657748dc5596000aebbe4f6ede3aed67\n",
      "Successfully built spacy-universal-sentence-encoder\n",
      "Installing collected packages: spacy-universal-sentence-encoder\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed spacy-universal-sentence-encoder-0.4.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets\n",
    "# !pip install \"tensorflow>=2.0.0\"\n",
    "# !pip install --upgrade tensorflow-hub\n",
    "# !pip install spacy-universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use the library to download (and load) the model.\n",
    "\n",
    "NB: model downloading may take time (depending on the model hosting). If you think it may take a long time, ask your TA for assistance with binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 11:49:00.843587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy_universal_sentence_encoder\n",
    "nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "embedd = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a function that prepares embedding of arbitrary queries\n",
    "\n",
    "Write a function, which returns a fixed-sized vector of embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    return embedd([text])[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "qwe = embed(\"Folks, here's a story about Minnie the Moocher. \")\n",
    "print(qwe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that embeddings are of the same size and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embed(\n",
    "            \"Some random text\"\n",
    "        ).shape == \\\n",
    "        embed(\n",
    "            \"Folks, here's a story about Minnie the Moocher. \"\n",
    "            \"She was a lowdown hoochie coocher. \"\n",
    "            \"She was the roughest, toughest frail, \"\n",
    "            \"but Minnie had a heart as big as a whale\"\n",
    "        ).shape, \"Shape should match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: here we check DISTANCE, not similarity. This similar texts should produce results close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"some text for testing\"), \n",
    "            embed(\"some text for testing\")\n",
    "        )) < 1e-4, \"Embedding should match\"\n",
    "\n",
    "assert abs(cosine(\n",
    "            embed(\"Cats eat mice.\"), \n",
    "            embed(\"Terminator is an autonomous cyborg, typically humanoid, originally conceived as a virtually indestructible soldier, infiltrator, and assassin.\")\n",
    "        )) > 0.2, \"Embeddings should be far\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the data\n",
    "\n",
    "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "facts = requests.get(url).text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
      "2. McDonalds calls frequent buyers of their food \"heavy users.\"\n",
      "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
      "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
      "5. You burn more calories sleeping than you do watching television.\n"
     ]
    }
   ],
   "source": [
    "print(*facts[:5], sep='\\n')\n",
    "\n",
    "assert len(facts) == 159\n",
    "assert ('our lovely little planet') in facts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform sentences to vectors\n",
    "\n",
    "Transform the list of facts to `numpy.array` of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#TODO infer vectors\n",
    "sent_vecs = np.array([embed(text) for text in facts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent_vecs.shape[0] == len(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find closest to the query\n",
    "\n",
    "Now find 5 facts which are the closest to the query using cosine measure.\n",
    "\n",
    "### 5.1. Closest search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_closest(query, dataset, k=10):\n",
    "    return np.argsort(dataset @ query)[-k:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Use your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: good mood\n",
      "\n",
      "\t 44. Honey never spoils.\n",
      "\t 45. About half of all Americans are on a diet on any given day.\n",
      "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
      "\t 68. Cherophobia is the fear of fun.\n",
      "\t 57. Gorillas burp when they are happy\n"
     ]
    }
   ],
   "source": [
    "query = \"good mood\"\n",
    "query_vec = embed(query)\n",
    "\n",
    "print(\"Results for query:\", query)\n",
    "print()\n",
    "for k in find_k_closest(query_vec, sent_vecs, 5):\n",
    "    print(\"\\t\", facts[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure DCG@5 for the following query bucket\n",
    "```\n",
    "good mood\n",
    "gorilla\n",
    "woman\n",
    "earth\n",
    "japan\n",
    "people\n",
    "math\n",
    "```\n",
    "\n",
    "Recommend 5 facts to each of the queries. Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "good mood\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 68. Cherophobia is the fear of fun.\n",
      "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
      "\t 45. About half of all Americans are on a diet on any given day.\n",
      "\t 44. Honey never spoils.\n",
      "\n",
      "\n",
      "gorilla\n",
      "\t 55. The word \"gorilla\" is derived from a Greek word meaning, \"A tribe of hairy women.\"\n",
      "\t 57. Gorillas burp when they are happy\n",
      "\t 137. Human birth control pills work on gorillas.\n",
      "\t 106. The male ostrich can roar just like a lion.\n",
      "\t 85. The elephant is the only mammal that can't jump!\n",
      "\n",
      "\n",
      "woman\n",
      "\t 151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
      "\t 16. Men are 6 times more likely to be struck by lightning than women.\n",
      "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
      "\t 55. The word \"gorilla\" is derived from a Greek word meaning, \"A tribe of hairy women.\"\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\n",
      "\n",
      "earth\n",
      "\t 88. Earth is the only planet that is not named after a god.\n",
      "\t 21. Earth has traveled more than 5,000 miles in the past 5 minutes.\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 147. Russia has a larger surface area than Pluto.\n",
      "\t 152. There are more stars in space than there are grains of sand on every beach in the world.\n",
      "\n",
      "\n",
      "japan\n",
      "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
      "\t 123. There are 5 temples in Kyoto, Japan that have blood stained ceilings. The ceilings are made from the floorboards of a castle where warriors killed themselves after a long hold-off against an army. To this day, you can still see the outlines and footprints.\n",
      "\t 64. In Japan, crooked teeth are considered cute and attractive.\n",
      "\t 147. Russia has a larger surface area than Pluto.\n",
      "\t 79. A waterfall in Hawaii goes up sometimes instead of down.\n",
      "\n",
      "\n",
      "people\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
      "\t 34. 95% of people text things they could never say in person.\n",
      "\t 154. The total weight of all those ants, however, is about the same as all the humans.\n",
      "\t 102. More than 50% of the people in the world have never made or received a telephone call.\n",
      "\t 87. If 33 million people held hands, they could make it all the way around the equator.\n",
      "\n",
      "\n",
      "math\n",
      "\t 119. Dogs are capable of understanding up to 250 words and gestures and have demonstrated the ability to do simple mathematical calculations.\n",
      "\t 132. If you started with $0.01 and doubled your money every day, it would take 27 days to become a millionaire.\n",
      "\t 30. If you were to stretch a Slinky out until it's flat, it would measure 87 feet long.\n",
      "\t 94. Of all the words in the English language, the word \"set\" has the most definitions. The word \"run\" comes in close second.\n",
      "\t 27. A mole can dig a tunnel that is 300 feet long in only one night.\n"
     ]
    }
   ],
   "source": [
    "bucket = \"\"\"good mood\n",
    "gorilla\n",
    "woman\n",
    "earth\n",
    "japan\n",
    "people\n",
    "math\"\"\".split('\\n')\n",
    "\n",
    "for term in bucket:\n",
    "    print(\"\\n\")\n",
    "    print(term)\n",
    "    for k in find_k_closest(embed(term), sent_vecs, k=5)[::-1]:\n",
    "        print(\"\\t\", facts[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write your own relevance assessments and compute DCG@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@5 = 1.5029\n",
      "IDCG@5 = 2.9485\n",
      "nDCG@5 = 0.5097\n"
     ]
    }
   ],
   "source": [
    "assessments = [\n",
    "    [1, 0, 0, 0, 0], # good mood\n",
    "    [1, 1, 1, 0, 0], # gorilla\n",
    "    [0, 0, 0, 0, 0], # ...\n",
    "    ...\n",
    "]\n",
    "\n",
    "def dcg(rels):\n",
    "    from math import log\n",
    "    s = 0\n",
    "    for i, rel int enumerate(rels):\n",
    "        s += rel / log(1 + i + 1, 2)\n",
    "    return s\n",
    "\n",
    "print(f\"DCG@5 = {dcg5:.4f}\")\n",
    "print(f\"IDCG@5 = {idcg5:.4f}\")\n",
    "print(f\"nDCG@5 = {dcg5 / idcg5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
